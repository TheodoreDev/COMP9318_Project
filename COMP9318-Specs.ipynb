{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP-9318 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions:\n",
    "1. This note book contains instructions for **COMP9318 Final-Project**.\n",
    "\n",
    "* You are required to complete your implementation in a file `submission.py` provided along with this notebook.\n",
    "\n",
    "* You are not allowed to print out unnecessary stuff. We will not consider any output printed out on the screen. All results should be returned in appropriate data structures returned by corresponding functions.\n",
    "\n",
    "* This notebook encompasses all the requisite details regarding the project. Detailed instructions including **CONSTRAINTS**, **FEEDBACK** and **EVALUATION** are provided in respective sections. In case of additional problem, you can post your query @ Piazza.\n",
    "\n",
    "* This project is **time-consuming**, so it is highly advised that you start working on this as early as possible.\n",
    "\n",
    "* You are allowed to use only the permitted libraries and modules (as mentioned in the **CONSTRAINTS** section). You should not import unnecessary modules/libraries, failing to import such modules at test time will lead to errors.\n",
    "\n",
    "* You are **NOT ALLOWED** to use dictionaries and/or external data resources for this project.\n",
    "\n",
    "* We will provide you **LIMITED FEEDBACK** for your submission (only **15** attempts allowed to each group). Instructions for the **FEEDBACK** and final submission are given in the **SUBMISSION** section.\n",
    "\n",
    "* For **Final Evaluation** we will be using a different dataset, so your final scores may vary.  \n",
    "\n",
    "* Submission deadline for this assignment is **23:59:59 on 27-May, 2018**.\n",
    "* **Late Penalty: 10-% on day-1 and 20% on each subsequent day.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "In this Project, you are required to devise an algorithm/technique to fool a binary classifier named `target-classifier`. In this regard, you only have access to following information:\n",
    "\n",
    "<br>\n",
    "1. The `target-classifier` is a binary classifier classifying data to two categories, $\\textit{i.e.}$, **class-1** and **class-0**.\n",
    "\n",
    "2. You have access to part of classifiers' training data, $\\textit{i.e.}$, a sample of 540 paragraphs. 180 for **class-1**, and 360 for **class-0**, provided in the files: `class-1.txt` and `class-0.txt` respectively.\n",
    "\n",
    "3. The `target-classifier` belong to the SVM family.\n",
    "\n",
    "4. The `target-classifier` allows **EXACTLY 20 DISTINCT** modifications in each test sample.\n",
    "5. You are provided with a test sample of **200** paragraphs from **class-1** (in the file: `test_data.txt`). You can use these test samples to get feedback from the target classifier (**only 15 attempts** allowed to each group.).\n",
    "6. **NOTE: You are not allowed to use the data `test_data.txt` for your model training (if any). VIOLATIONS in this regard will get ZERO score**.\n",
    "\n",
    "<br>\n",
    "### -to-do:\n",
    "* You are required to come up with an algorithm named `fool_classifier()` that makes best use of the above-mentioned information (**point 1-4**) to fool the `target-classifier`. By fooling the classifier we mean that your algorithm can help mis-classify a bunch of test instances (**point-5**) with minimal possible modifications (**EXACTLY 20 DISTINCT** modifications allowed to each test sample). \n",
    "\n",
    "* **NOTE::** We put a **harsh limit** on the number of modifications allowed for each test instance. You are only allowed to modify each test sample by **EXACTLY 20 DISTINCT tokens (NO MORE NO LESS)**.\n",
    "\n",
    "* **NOTE::** **ADDING** or **DELETING** one word at a time is **ONE** modification. Replacement will be considered as **TWO** modifications $(\\textit{i.e.,}$ **Deletion** followed by **Insertion**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints\n",
    "\n",
    "Your implementation `submission.py` should comply with following constraints.\n",
    "\n",
    "1. You should implement your methodology using `Python3`.\n",
    "* You should implement your code in the function `fool_classifier()` in the file `submission.py`. \n",
    "* You are only allowed to use pre-defined class `strategy()` defined in the file: `helper.py` in order to train your models (if any). \n",
    "* You **should not** do any pre-processing on the data. We have already pre-processed the data for you.  \n",
    "* You are supposed to implement your algorithm using **scikit-learn (version=0.19.1)**. We will **NOT** accept implementations using other Libraries.\n",
    "\n",
    "* You are **not supposed to augment** the data using external/additional resources.  You are only allowed to use the partial training data provided to you ($\\textit{i.e.,} $ `class-1.txt` and `class-0.txt`).\n",
    "\n",
    "* You are **not** allowed to use the test samples ($\\textit{i.e.,}$ `test_data.txt`) for model training and/or inference building. You can only use this data for testing, $\\textit{i.e.,}$ calculating success %-age (as described in the **EVALUATION** section.). **VIOLATIONS IN THIS REGARD WILL GET ZERO SCORE**.\n",
    "\n",
    "* You are **not** allowed to hard code the ground truth and any other information into your implementation `submission.py`. \n",
    "\n",
    "* Considering the **RUNNING TIME**, your implementation is supposed to read the test data file ($\\textit{i.e.,}$ `test_data.txt` with 200 test samples), process it and write the modified file (`modified_data.txt`) within **12 Minutes**.\n",
    "\n",
    "* Each modified test sample in the modified file (`modified_data.txt`) should not differ from the original test sample corresponding to the file (`test_data.txt`) by more than 20 tokens.\n",
    "\n",
    "* **NOTE::** Inserting or Deleting a word is **ONE** modification. Replacement will be considered as **TWO** modifications $(\\textit{i.e.,}$ deletion followed by insertion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions:\n",
    "\n",
    "* Please read these instructions **VERY CAREFULLY**.\n",
    "\n",
    "### FEEDBACK:\n",
    "* For this project, we will provide real-time feed-back on a test data ($\\textit{i.e.,}$ the file `test_data.txt` containing **200** test cases).\n",
    "* Each group is allowed to avail only **15 attempts in TOTAL**, so use your attempts **WISELY**.\n",
    "* We will only provide **ACCUMULATIVE FEEDBACK** ($\\textit{i.e.,}$ how many modified test samples out of **200** were classified as Class-0). We **WILL NOT** provide detailed feedback for individual test cases.\n",
    "* For the feedback, you are required to submit the modified text file ($\\textit{i.e.,}$ `modified_data.txt`) via the submission portal: http://kg.cse.unsw.edu.au:8318/project/ (using Group name and Group password).\n",
    "* **NOTE::** Please make sure that the modified text file is generated by your program `fool_classifier()`, and it obeys the modification constraints. We have provided a function named: `check_data()` in the class: `strategy()`to check whether the modified file: `modified_data.txt` obeys the constraints.\n",
    "\n",
    "3. Your algorithm should modify each test sample in `test_data.txt` by **EXACTLY 20 DISTINCT TOKENS**.\n",
    "\n",
    "### Final Submission:\n",
    "1. For final submission, you need to submit:\n",
    "    * Your code in the file `submission.py`\n",
    "    * A report (`report.pdf`) outlining your approach for this project.\n",
    "2. We will release the detailed instructions for the final submission submission via Piazza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "1. In the file `submission.py`, you are required to implement a function named: `fool_classifier()` that reads a text file named: `test_data.txt` from Present Working Directory(PWD), and writes out the modified text file: `modified_data.txt` in the same directory.\n",
    "* We have provided the implementation of **strategy** class in a seperate file `helper.py`. You are supposed to use this class for your model training (if any) and inference building.\n",
    "\n",
    "* **Detailed description of input and/or output parts is given below:**\n",
    "\n",
    "### Input: \n",
    "* The function `fool_classifier()` reads a text files named `test_data.txt` having almost (500-1500) test samples. Each line in the input file corresponds to a single test sample.\n",
    "\n",
    "* **Note:** We will also provide the partial training data ($\\textit{(i)}$ `class-0.txt` and $\\textit{(ii)}$ `class-1.txt`) in the test environment. You can  access this data using the class: `strategy()`. \n",
    "\n",
    "### Output:\n",
    "* You are supposed to write down the modified file named `modified_data.txt` in the same directory, and in the same format as that of the `test_data.txt`. In addition, your program is supposed to return the instance of the `strategy` class defined in `helper.py`.\n",
    "\n",
    "\n",
    "* **Note:** Please make sure that the file: `modified_data.txt` is generated by your code, and it follows the **MODIFICATION RESTRICTIONS (ADD** and/or **DELETE EXACTLY 20 DISTINCT TOKENS)**. In case of **ERRORS**, we will **NOT** allow more feedback attempts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have provided these implementations in the file helper.py, provided along with this project.\n",
    "## Please do not change these functions.\n",
    "###################\n",
    "class countcalls(object):\n",
    "    __instances = {}\n",
    "    def __init__(self, f):\n",
    "        self.__f = f\n",
    "        self.__numcalls = 0\n",
    "        countcalls.__instances[f] = self\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        self.__numcalls += 1\n",
    "        return self.__f(*args, **kwargs)\n",
    "    @staticmethod\n",
    "    def count(f):\n",
    "        return countcalls.__instances[f].__numcalls\n",
    "    @staticmethod\n",
    "    def counts():\n",
    "        res = sum(countcalls.count(f) for f in countcalls.__instances)\n",
    "        for f in countcalls.__instances:\n",
    "            countcalls.__instances[f].__numcalls = 0\n",
    "        return res\n",
    "    \n",
    "## Strategy() class provided in helper.py to facilitate the implementation.\n",
    "class strategy:\n",
    "    ## Read in the required training data...\n",
    "    def __init__(self):\n",
    "        with open('class-0.txt','r') as class0:\n",
    "            class_0=[line.strip().split(' ') for line in class0]\n",
    "        with open('class-1.txt','r') as class1:\n",
    "            class_1=[line.strip().split(' ') for line in class1]\n",
    "        self.class0=class_0\n",
    "        self.class1=class_1\n",
    "    \n",
    "    @countcalls\n",
    "    def train_svm(parameters, x_train, y_train):\n",
    "        ## Populate the parameters...\n",
    "        gamma=parameters['gamma']\n",
    "        C=parameters['C']\n",
    "        kernel=parameters['kernel']\n",
    "        degree=parameters['degree']\n",
    "        coef0=parameters['coef0']\n",
    "        \n",
    "        ## Train the classifier...\n",
    "        clf = svm.SVC(kernel=kernel, C=C, gamma=gamma, degree=degree, coef0=coef0)\n",
    "        assert x_train.shape[0] <=541 and x_train.shape[1] <= 5720\n",
    "        clf.fit(x_train, y_train)\n",
    "        return clf\n",
    "    \n",
    "    ## Function to check the Modification Limits...(You can modify EXACTLY 20-DISTINCT TOKENS)\n",
    "    def check_data(self, original_file, modified_file):\n",
    "        with open(original_file, 'r') as infile:\n",
    "            data=[line.strip().split(' ') for line in infile]\n",
    "        Original={}\n",
    "        for idx in range(len(data)):\n",
    "            Original[idx] = data[idx]\n",
    "\n",
    "        with open(modified_file, 'r') as infile:\n",
    "            data=[line.strip().split(' ') for line in infile]\n",
    "        Modified={}\n",
    "        for idx in range(len(data)):\n",
    "            Modified[idx] = data[idx]\n",
    "                    \n",
    "        for k in sorted(Original.keys()):\n",
    "            record=set(Original[k])\n",
    "            sample=set(Modified[k])\n",
    "            assert len((set(record)-set(sample)) | (set(sample)-set(record)))==20\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training Data\n",
    "-There is two version of bag of words:\n",
    "1. Bag of Words created by myself(Accuracy against test case = 51.5%\n",
    "2. Bag of Words created by CountVectorizer(Accuracy against test case = 49%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn import svm\n",
    "\n",
    "def get_freq_of_tokens(ls):\n",
    "    tokens = {}\n",
    "    for token in ls:\n",
    "        if token not in tokens:\n",
    "            tokens[token] = 1\n",
    "        else:\n",
    "            tokens[token] += 1\n",
    "    return tokens\n",
    "\n",
    "#Training SVM (for testing purpose)\n",
    "training = strategy()\n",
    "\n",
    "#data provided in strategy class\n",
    "\n",
    "#turn data to bag of words (refer to wikipedia or other sources)\n",
    "\n",
    "#make each distinct word into a set\n",
    "features = set()\n",
    "for i in training.class0:\n",
    "    features = features | set(i)\n",
    "\n",
    "for i in training.class1:\n",
    "    features = features | set(i)\n",
    "\n",
    "features = list(features) #changed so the features is in order and can be used for other purpose\n",
    "\n",
    "#creating a dict per list (per doc)\n",
    "newlist0 = []\n",
    "for i in training.class0:\n",
    "    newlist0.append(get_freq_of_tokens(i))\n",
    "\n",
    "#print(newlist0)\n",
    "\n",
    "newlist1 = []\n",
    "for i in training.class1:\n",
    "    newlist1.append(get_freq_of_tokens(i))\n",
    "\n",
    "newdict = dict.fromkeys(features, 0)\n",
    "\n",
    "#make a list of dict representing the whole data\n",
    "xdata = []\n",
    "for row in newlist0:\n",
    "    tmp_dict = dict(newdict)\n",
    "    for i in row:\n",
    "        if i in newdict:\n",
    "            tmp_dict[i]+= row[i]\n",
    "    xdata.append(tmp_dict)\n",
    "    \n",
    "\n",
    "    \n",
    "for row in newlist1:\n",
    "    tmp_dict = dict(newdict)\n",
    "    for i in row:\n",
    "        if i in newdict:\n",
    "            tmp_dict[i]+= row[i]\n",
    "    xdata.append(tmp_dict)\n",
    "\n",
    "ydata = []\n",
    "for i in range(len(newlist0)):\n",
    "    ydata.append(0)\n",
    "for i in range(len(newlist1)):\n",
    "    ydata.append(1)\n",
    "\n",
    "x_data = pandas.DataFrame(xdata)\n",
    "#x_train, and y_train done\n",
    "#or create a df with index (if you know exactly the number of rows), the use df.loc[x] to input the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING COUNTVECTORIZER\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#with pipeline (DOESN'T WORK YET)\n",
    "#pipe = Pipeline(['vect', CountVectorizer(), 'tfidf', TfidfTransformer()])\n",
    "#trainXpipe = pipe.fit_transform(combinedclass,ydata)\n",
    "\n",
    "#without pipeline\n",
    "vectorizer = CountVectorizer()\n",
    "newclass0 = [' '.join(x) for x in training.class0]\n",
    "newclass1 = [' '.join(x) for x in training.class1]\n",
    "combinedclass = newclass0 + newclass1\n",
    "\n",
    "trainX = vectorizer.fit_transform(combinedclass)\n",
    "#print(trainX.shape)\n",
    "\n",
    "#Using TFI-DF Transformation\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidftrainX = tfidf_transformer.fit_transform(trainX)\n",
    "#print(tfidftrainX.shape)\n",
    "\n",
    "#creating pipeline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "There is two option, using countvectorizer and our bag of words\n",
    "\n",
    "### NOTE: Don't forget to comment out one of the model below so it will works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training SVM using default parameter\n",
    "parameter = {'gamma': 'auto', 'C': 1.0 ,'kernel': 'linear','degree': 3 ,'coef0': 0.0}\n",
    "\n",
    "#using CountVectorizer bag of words\n",
    "#model = training.train_svm(parameter, trainX, ydata)\n",
    "\n",
    "#using CountVectorizer + TFI-DF Transformer\n",
    "#model = training.train_svm(parameter, tfidftrainX, ydata)\n",
    "\n",
    "#using own bag of words\n",
    "model = training.train_svm(parameter, x_data, ydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Accuracy against test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING OWN BAG OF WORDS\n",
    "#check SVM against test data\n",
    "test_data = \"D:/MyStudy/Data Warehousing & Data Mining/Project for Data Warehousing/test_data.txt\"\n",
    "#open the test data\n",
    "with open(test_data, \"r+\") as tdata:\n",
    "    \n",
    "    #check inside of test_data\n",
    "    #for i in tdata:\n",
    "        #print(i)\n",
    "\n",
    "    #change test data into list of list\n",
    "    newlist = []\n",
    "    for i in tdata:\n",
    "        alist = i.split(' ')\n",
    "        newlist.append(alist)\n",
    "    \n",
    "    #make list of list into list of dict\n",
    "    listdict = list()\n",
    "    for i in newlist:\n",
    "        listdict.append(get_freq_of_tokens(i))\n",
    "    \n",
    "    #make features\n",
    "    #features = set()\n",
    "    #for i in newlist:\n",
    "    #    features = features|set(i)\n",
    "    \n",
    "    #make X_test\n",
    "    testdict = dict.fromkeys(features, 0)\n",
    "    \n",
    "    X_test = list()\n",
    "    for i in listdict:\n",
    "        tempdict = dict(testdict)\n",
    "        for wrd in i:\n",
    "            if wrd in tempdict:\n",
    "                tempdict[wrd] += i[wrd]\n",
    "        X_test.append(tempdict)\n",
    "    \n",
    "    #checking frequency in X_test\n",
    "    #count = 0\n",
    "    #for i in X_test:\n",
    "    #    for key, val in i.items():\n",
    "    #        if val > 0:\n",
    "    #            count += 1\n",
    "    #            print(f\"There is {count}\")\n",
    "                \n",
    "    Xtest = pandas.DataFrame(X_test)\n",
    "    \n",
    "    #make y_test\n",
    "    ytest = [1]*len(newlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING CountVectorizer Bag of Words\n",
    "#check SVM against test data\n",
    "test_data = \"D:/MyStudy/Data Warehousing & Data Mining/Project for Data Warehousing/test_data.txt\"\n",
    "#open the test data\n",
    "with open(test_data, \"r+\") as tdata:\n",
    "\n",
    "    #change test data into list of list\n",
    "    newlist = []\n",
    "    for i in tdata:\n",
    "        alist = i.split(' ')\n",
    "        newlist.append(alist)\n",
    "    \n",
    "    #make testX using CountVectorizer\n",
    "    datatest = [' '.join(x) for x in newlist]\n",
    "    \n",
    "    testX = vectorizer.transform(datatest)\n",
    "    newtestX = tfidf_transformer.transform(testX)\n",
    "                    \n",
    "    #make y_test\n",
    "    ytest = [1]*len(newlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM against training data = 1.0\n",
      "SVM prediction against test data = [1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1\n",
      " 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0\n",
      " 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0\n",
      " 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0\n",
      " 0 1 0 1 1 0 0 0 0 1 1 0 0 1 1]\n",
      "SVM against test data = 0.515\n"
     ]
    }
   ],
   "source": [
    "#check SVM against training data\n",
    "\n",
    "#this is for CountVectorizer Bag of Words\n",
    "#print(f\"SVM against training data = {model.score(trainX, ydata)}\")\n",
    "#print(f\"SVM prediction against test data = {model.predict(testX)}\")\n",
    "#print(f\"SVM against test data = {model.score(testX, ytest)}\")     \n",
    "\n",
    "#this is for CountVectorizer + TFI-DF Transformation \n",
    "#print(f\"SVM against training data = {model.score(tfidftrainX, ydata)}\")\n",
    "#print(f\"SVM prediction against test data = {model.predict(newtestX)}\")\n",
    "#print(f\"SVM against test data = {model.score(newtestX, ytest)}\")   \n",
    "\n",
    "#this is for our own Bag of Words\n",
    "print(f\"SVM against training data = {model.score(x_data, ydata)}\")\n",
    "print(f\"SVM prediction against test data = {model.predict(Xtest)}\")\n",
    "print(f\"SVM against test data = {model.score(Xtest, ytest)}\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding out top 20 features\n",
    "Using absolute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[3504, 4044, 2812, 9, 591, 5200, 13, 354, 4516, 3561, 968, 519, 5040, 2296, 3297, 958, 10, 1203, 2810, 2550]\n",
      "20\n",
      "['deteriorate', 'speaker', 'eat', 'rein', 'bissau', 'allow', 'st.', 'morning', '2001', 'often', 'turki', '63', 'registration', 'moya', 'thomas', 'actress', \"shi'ite\", 'flee', 'impeachment', 'tranmere']\n",
      "20\n",
      "[('deteriorate', 0.512758277494543), ('speaker', 0.34436707970255476), ('eat', -0.2671745602160941), ('rein', 0.2362104633966918), ('bissau', 0.23380898430670333), ('allow', -0.21773008031372137), ('st.', 0.21460421201154825), ('morning', 0.19597609145466482), ('2001', 0.18330528069383328), ('often', -0.17957078613374533), ('turki', 0.17046027506944586), ('63', 0.16979683742908908), ('registration', -0.1696792810947738), ('moya', 0.16607738709055733), ('thomas', 0.16453645539946735), ('actress', -0.16206514476857625), (\"shi'ite\", 0.16172986483887877), ('flee', 0.16061582201546684), ('impeachment', -0.15360940352282537), ('tranmere', 0.15303251780510055)]\n"
     ]
    }
   ],
   "source": [
    "weights = model.coef_.tolist()\n",
    "\n",
    "#apply abs to all of weights\n",
    "newweights = [abs(x) for x in weights[0]]\n",
    "\n",
    "#finding top 20\n",
    "sortweights = list(reversed(sorted(newweights)))\n",
    "top = sortweights[0:20]\n",
    "idx_top = list()\n",
    "for i in range(20):\n",
    "    idx = newweights.index(top[i])\n",
    "    idx_top.append(idx)  \n",
    "print(len(idx_top))\n",
    "print(idx_top)\n",
    "\n",
    "#finding out top 20 features words\n",
    "top_words = list()\n",
    "for i in range(20):\n",
    "    top_words.append(features[idx_top[i]])\n",
    "\n",
    "print(len(top_words))\n",
    "print(top_words)\n",
    "\n",
    "#pair up words with their real coefficient\n",
    "top_words_coef = list()\n",
    "for i in range(20):\n",
    "    top_words_coef.append((top_words[i],weights[0][idx_top[i]]))\n",
    "\n",
    "top_dict = dict(top_words_coef)\n",
    "\n",
    "print(len(top_words_coef))\n",
    "print(top_words_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorted List of Coefficient & Their Indexes\n",
    "- Instead of just finding top 20 features, I planned to sorted the whole coefficient and put the original index to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5718\n",
      "[3504, 4044, 2812, 9, 591, 5200, 13, 354, 4516, 3561, 968, 519, 5040, 2296, 3297, 958, 10, 1203, 2810, 2550, 3469, 26, 1313, 2590, 2196, 3657, 17, 776, 1828, 476, 1645, 1054, 5123, 2347, 2535, 3723, 4056, 439, 881, 5711, 15, 2210, 1204, 2063, 1315, 5173, 4897, 951, 5643, 1185, 5139, 4079, 3407, 3711, 2071, 2519, 1380, 5566, 1343, 454, 2789, 3480, 4923, 1888, 4119, 3034, 997, 3893, 2274, 3702, 4960, 5138, 2307, 1238, 4881, 516, 675, 3603, 2830, 2017, 5633, 4466, 2311, 3884, 736, 3440, 141, 4010, 541, 3802, 1060, 262, 2882, 4437, 5319, 5674, 1585, 389, 2013, 4046, 361, 679, 3604, 4917, 1825, 4963, 5539, 4422, 4553, 2961, 247, 5177, 3352, 3, 1640, 1835, 4141, 1547, 4594, 2779, 2136, 1758, 5492, 5591, 4065, 3155, 3685, 1034, 2832, 3252, 226, 3530, 2540, 2955, 2584, 3978, 4115, 4607, 4987, 3983, 3091, 4149, 1752, 4040, 4082, 4749, 1855, 3807, 2669, 4101, 3734, 4737, 3229, 94, 286, 2697, 64, 2183, 438, 1245, 3986, 2107, 472, 475, 4745, 3498, 5602, 4023, 4941, 738, 614, 3869, 2491, 1519, 2326, 752, 4290, 1277, 5683, 4828, 5335, 1275, 2596, 179, 5158, 1220, 4319, 174, 1764, 2487, 4454, 737, 4632, 1820, 1048, 4358, 3726, 1541, 2040, 1376, 1227, 2079, 5205, 4460, 1239, 1549, 1644, 1574, 407, 2681, 3923, 904, 5175, 1491, 1160, 4814, 4398, 4063, 905, 3613, 3070, 5671, 3668, 719, 1544, 4024, 3265, 681, 4374, 1324, 3683, 2568, 3834, 3509, 1827, 5096, 2880, 618, 3094, 1670, 4927, 3522, 619, 44, 1860, 4606, 912, 2045, 5476, 3108, 2197, 1796, 605, 798, 5195, 2422, 1567, 5619, 1690, 5415, 4022, 2523, 3089, 583, 5198, 1510, 5201, 891, 2871, 4768, 1812, 3093, 2708, 3228, 1685, 1997, 3597, 1816, 3575, 3396, 269, 3777, 2928, 3508, 2738, 4163, 5132, 1088, 1472, 4195, 3325, 5083, 1439, 608, 60, 1016, 3390, 3492, 3126, 2414, 3681, 1982, 4017, 3585, 1599, 1326, 4303, 3528, 2909, 1233, 5377, 4256, 526, 4970, 5542, 3952, 1910, 1179, 4604, 5495, 2495, 2440, 2558, 5222, 4348, 3319, 2991, 4325, 642, 5618, 5563, 1621, 3643, 4919, 1100, 4379, 5255, 2544, 3832, 3962, 523, 4670, 5538, 657, 2711, 3967, 1681, 2766, 3591, 3812, 4893, 5620, 813, 135, 1450, 5655, 3848, 1323, 88, 1490, 2135, 3300, 2663, 913, 2475, 197, 933, 3068, 5045, 2994, 961, 4784, 3859, 2640, 3282, 5681, 2824, 162, 4891, 1240, 4639, 3588, 4269, 1350, 4730, 1454, 16, 1518, 5081, 5031, 2186, 2186, 1333, 3484, 977, 2238, 1057, 4317, 1733, 3266, 3770, 5345, 865, 1869, 391, 2813, 2489, 3222, 1049, 5467, 2867, 3692, 5559, 5396, 2015, 817, 549, 2525, 5323, 3733, 2262, 2398, 2632, 3965, 2627, 1761, 2336, 1853, 5513, 1523, 153, 5095, 2814, 5544, 1459, 1201, 2253, 1082, 2866, 3449, 3075, 1823, 2293, 4161, 2177, 3964, 1251, 3935, 3625, 4211, 3445, 3938, 35, 3096, 2543, 146, 5678, 1389, 4750, 4413, 546, 603, 855, 3240, 3240, 1112, 4835, 4584, 4922, 482, 3238, 3069, 3327, 5630, 4289, 1524, 4862, 3957, 3081, 3438, 1911, 1911, 1911, 1911, 5023, 1228, 3290, 5015, 3576, 3811, 5237, 5300, 4883, 5080, 254, 1311, 785, 2703, 4953, 1668, 2862, 1084, 1354, 3356, 2206, 1357, 1438, 3018, 4631, 4187, 2829, 143, 2840, 1041, 3101, 3998, 2834, 2217, 3644, 780, 1650, 687, 3642, 1384, 5576, 460, 2984, 2964, 4230, 5182, 3631, 4977, 1484, 3210, 154, 2309, 5468, 4701, 2954, 1775, 5573, 5056, 4705, 2341, 510, 4387, 3309, 3977, 155, 4417, 5388, 5597, 1132, 1327, 176, 1562, 1815, 1815, 569, 716, 2916, 4568, 4568, 78, 1671, 2894, 5326, 1025, 1025, 1025, 1025, 995, 2088, 4505, 182, 2426, 3780, 3697, 2990, 2366, 3121, 5280, 4072, 5401, 5046, 1023, 1909, 5210, 1431, 3867, 3731, 1540, 4625, 3529, 1018, 5188, 2757, 2319, 5556, 191, 1476, 1476, 1476, 1476, 1476, 5186, 1661, 4248, 3389, 644, 2146, 5094, 5240, 3630, 3458, 1262, 4712, 3614, 2303, 992, 930, 3764, 972, 5512, 5486, 506, 3699, 5717, 2007, 2605, 4391, 3241, 626, 4359, 5598, 2509, 157, 2351, 4983, 12, 707, 4971, 784, 4564, 2664, 5254, 4565, 66, 66, 66, 3590, 4300, 1747, 5609, 4513, 4011, 1382, 4829, 3684, 998, 998, 998, 2593, 2641, 4448, 3391, 5660, 700, 2582, 4853, 3357, 3739, 5702, 4122, 4192, 4390, 920, 1921, 3635, 2533, 3852, 3559, 2317, 4889, 3506, 3634, 3694, 1895, 1851, 1610, 3907, 811, 1413, 4005, 1939, 4043, 5277, 1507, 4216, 1371, 2153, 2190, 1989, 1127, 3976, 2765, 355, 231, 2484, 1745, 816, 3773, 1912, 380, 3693, 1792, 470, 4322, 2712, 682, 2434, 685, 36, 5365, 3295, 250, 250, 4990, 329, 434, 3645, 3926, 1029, 4155, 455, 3340, 4621, 1122, 5499, 4645, 3071, 4020, 1833, 3306, 5259, 2891, 3676, 142, 4558, 4263, 4640, 1969, 5234, 1456, 303, 4458, 1124, 1164, 467, 202, 1617, 2288, 5339, 1583, 1329, 4900, 874, 874, 874, 874, 874, 874, 2411, 931, 931, 931, 931, 2349, 577, 2826, 4133, 2799, 4614, 1557, 1320, 5269, 1501, 4830, 897, 464, 803, 1208, 1177, 1721, 4418, 5137, 5594, 2041, 4842, 1167, 54, 4213, 2515, 3176, 5105, 5108, 5667, 3467, 5239, 1093, 5584, 4964, 2245, 1770, 1770, 2089, 5212, 2273, 1861, 3658, 1051, 1839, 3901, 1437, 604, 3839, 4665, 373, 3944, 4272, 3861, 4515, 4381, 5060, 1750, 5026, 3209, 1843, 4360, 20, 20, 20, 20, 20, 20, 2006, 5502, 161, 804, 804, 4662, 2083, 4844, 5285, 5318, 2648, 4375, 3359, 914, 914, 914, 914, 914, 914, 158, 697, 5058, 1276, 1539, 5575, 5590, 2937, 3133, 1633, 2368, 4596, 916, 1503, 636, 2649, 383, 383, 383, 383, 383, 383, 383, 2123, 2229, 4821, 2403, 2403, 2403, 3730, 3730, 3730, 133, 1086, 3574, 1206, 3891, 2144, 1762, 5482, 4783, 984, 5599, 4796, 1808, 1808, 1808, 1808, 1808, 1808, 1808, 1808, 3099, 3680, 4974, 733, 733, 659, 659, 659, 659, 659, 3705, 4232, 854, 5030, 86, 86, 86, 86, 74, 1605, 1605, 2412, 3041, 3747, 3835, 5337, 5558, 4318, 4754, 5109, 2556, 2556, 2556, 910, 3432, 1387, 4573, 3272, 1986, 2872, 2027, 2030, 4031, 870, 462, 2187, 3892, 3979, 1297, 4029, 757, 451, 903, 450, 5574, 647, 5511, 2109, 2907, 2736, 2431, 878, 5129, 4296, 375, 375, 375, 375, 505, 505, 2166, 1578, 1356, 1356, 1356, 1356, 1356, 1356, 1356, 858, 629, 629, 629, 629, 629, 629, 629, 4109, 2096, 2096, 2096, 2096, 2096, 1872, 4653, 3413, 5199, 2571, 442, 3865, 125, 4980, 2823, 859, 4567, 1190, 1190, 1190, 1190, 1190, 1190, 1190, 1190, 1190, 4811, 1900, 3127, 5490, 5310, 2227, 4320, 5062, 2462, 1072, 213, 1322, 2181, 2181, 1743, 4873, 4910, 1015, 4278, 5536, 2577, 4664, 720, 5068, 1142, 1142, 1142, 1142, 1142, 1647, 307, 307, 307, 307, 307, 307, 307, 4616, 5376, 3567, 2695, 5487, 3911, 2993, 273, 5016, 1511, 5293, 4212, 2436, 962, 676, 676, 676, 676, 676, 944, 4123, 396, 396, 396, 396, 396, 396, 396, 3046, 2141, 3784, 671, 3063, 5150, 2103, 5307, 986, 3379, 1834, 2415, 1550, 1441, 2644, 533, 2603, 532, 3092, 2044, 3303, 3303, 3303, 3303, 1502, 3354, 1194, 558, 558, 558, 3142, 2310, 4221, 2610, 3017, 2467, 2467, 2467, 2467, 5308, 2087, 3183, 424, 514, 3235, 3743, 463, 2586, 2438, 85, 85, 85, 85, 85, 85, 85, 85, 1865, 2099, 536, 308, 308, 308, 308, 1105, 2080, 2080, 1980, 616, 2667, 1111, 2896, 5483, 4818, 4252, 1732, 4925, 2384, 1153, 3061, 1891, 1891, 1891, 1891, 1891, 1891, 1891, 4717, 1526, 5583, 2205, 1811, 3441, 4107, 883, 883, 872, 1659, 860, 2416, 5405, 2072, 3296, 233, 4578, 3253, 3066, 87, 87, 87, 87, 87, 87, 87, 1973, 457, 457, 457, 457, 457, 457, 457, 457, 457, 457, 457, 722, 5351, 5613, 899, 899, 899, 899, 899, 899, 899, 5380, 5457, 4369, 372, 1651, 1651, 1651, 1651, 1651, 1651, 1651, 5007, 5338, 5433, 5262, 5053, 973, 985, 4087, 1926, 5460, 2827, 1126, 3571, 983, 5181, 3452, 2966, 3881, 5477, 461, 461, 461, 461, 1221, 1832, 4371, 1704, 5333, 421, 421, 421, 421, 421, 421, 1101, 5392, 2463, 5394, 3344, 2986, 3461, 3418, 3757, 2895, 5662, 1361, 2433, 3682, 382, 1043, 3715, 3595, 2425, 164, 1107, 2034, 392, 2620, 1581, 207, 4509, 5258, 5196, 5592, 1516, 672, 612, 1298, 3549, 4708, 1028, 363, 820, 820, 820, 5231, 1337, 2600, 2798, 353, 353, 353, 353, 1596, 5426, 4586, 3988, 32, 4903, 5265, 5696, 5039, 4062, 5167, 1481, 1652, 4306, 4463, 83, 83, 83, 83, 83, 5713, 1744, 4052, 3077, 4809, 3675, 4006, 4108, 31, 5366, 5451, 773, 3510, 3141, 2749, 1230, 3287, 715, 715, 715, 715, 715, 715, 715, 715, 715, 351, 755, 5675, 3525, 4455, 3483, 491, 1856, 1856, 1856, 1856, 1856, 5035, 217, 2578, 2235, 4061, 4061, 5051, 4659, 3570, 5042, 1706, 509, 55, 55, 55, 55, 55, 55, 55, 55, 55, 3145, 1209, 1520, 943, 943, 943, 943, 943, 943, 5128, 902, 1622, 4615, 3569, 2090, 4328, 990, 5145, 335, 335, 335, 5341, 296, 296, 296, 296, 296, 296, 296, 296, 1678, 819, 819, 819, 819, 819, 819, 819, 819, 819, 1892, 4598, 4181, 3776, 4198, 3920, 1806, 3177, 2473, 5074, 5363, 387, 387, 387, 387, 387, 4628, 964, 964, 964, 964, 964, 964, 964, 964, 964, 964, 742, 1830, 5043, 4152, 3636, 2637, 2345, 1288, 1654, 406, 3719, 1493, 5193, 1334, 2313, 466, 732, 1129, 1129, 3951, 4495, 1829, 4310, 3870, 3444, 2797, 5687, 1727, 4500, 2837, 1579, 978, 978, 978, 978, 978, 1863, 4667, 1336, 3984, 2203, 2203, 2203, 2203, 2203, 2203, 2203, 2203, 2203, 2203, 2203, 50, 2464, 3572, 2456, 4968, 2560, 5170, 5666, 1798, 1673, 5322, 808, 3916, 2173, 341, 4958, 3401, 560, 568, 568, 568, 568, 568, 568, 2039, 2453, 2453, 712, 390, 713, 573, 5634, 1777, 1933, 2035, 1538, 955, 955, 955, 955, 955, 955, 955, 5610, 3499, 832, 832, 832, 832, 832, 832, 3690, 3140, 1360, 1360, 1360, 1360, 2201, 68, 68, 68, 68, 68, 68, 120, 3587, 4630, 749, 749, 749, 749, 749, 3727, 19, 19, 19, 19, 19, 19, 19, 655, 4416, 30, 800, 800, 800, 800, 800, 3611, 4774, 2583, 3291, 3364, 4352, 3853, 835, 2145, 1352, 1451, 5435, 1669, 5427, 1525, 1168, 1065, 960, 3973, 2805, 1135, 5446, 2781, 5190, 863, 3298, 5171, 652, 5248, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 1309, 4304, 2193, 4720, 4009, 1241, 384, 384, 2442, 1513, 3921, 69, 4378, 2053, 3451, 1686, 1154, 4074, 1115, 1115, 1115, 1115, 5353, 2160, 508, 2298, 4293, 1725, 1725, 1725, 1725, 1725, 1725, 3847, 592, 592, 592, 592, 592, 927, 3640, 5475, 3882, 4863, 579, 3742, 1370, 5455, 3355, 1191, 1191, 1191, 1191, 1191, 1191, 1191, 1191, 1191, 1191, 1191, 209, 688, 367, 5614, 2742, 5367, 3564, 3564, 3381, 3206, 1106, 696, 696, 696, 696, 696, 696, 4106, 2023, 2555, 1971, 554, 554, 554, 5638, 4504, 2625, 1729, 201, 201, 201, 201, 201, 201, 201, 201, 4162, 641, 641, 641, 641, 378, 378, 378, 378, 2122, 1995, 4286, 548, 3761, 4918, 497, 497, 497, 497, 497, 497, 5325, 1782, 3102, 1435, 4685, 459, 140, 370, 370, 370, 370, 370, 370, 370, 370, 370, 370, 3539, 1264, 4914, 3639, 47, 121, 5121, 116, 116, 116, 116, 116, 116, 116, 116, 1672, 2081, 2081, 2168, 845, 4270, 105, 597, 4948, 489, 851, 5360, 5343, 425, 425, 425, 425, 425, 425, 425, 425, 3180, 4539, 4403, 5252, 1691, 1691, 1691, 1691, 1582, 1390, 1390, 1390, 1390, 1390, 660, 2958, 204, 4965, 1405, 2759, 305, 305, 305, 940, 940, 940, 940, 940, 940, 2297, 5288, 2930, 2930, 2930, 2930, 2930, 4791, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 330, 2645, 5653, 840, 5154, 53, 2786, 5560, 2339, 2339, 2339, 2339, 2339, 2339, 2339, 453, 2920, 726, 4368, 3647, 2114, 3020, 3606, 4581, 965, 2542, 2258, 5501, 5611, 2646, 5009, 4058, 4988, 3654, 1667, 38, 38, 38, 38, 38, 38, 428, 428, 428, 428, 428, 5654, 4324, 5235, 4735, 1649, 628, 628, 628, 628, 628, 628, 628, 628, 628, 628, 511, 343, 343, 5122, 2290, 613, 3615, 3584, 230, 3500, 3601, 3403, 710, 4112, 3366, 3366, 2142, 4428, 1374, 3311, 593, 5399, 1682, 62, 62, 62, 62, 62, 62, 5354, 2997, 5384, 4867, 2704, 4299, 4350, 1255, 2722, 3729, 2199, 4855, 4624, 3411, 3768, 4951, 5684, 3580, 276, 908, 3175, 2172, 4661, 3221, 1260, 4711, 2073, 2486, 5385, 440, 540, 46, 46, 46, 46, 46, 46, 46, 1887, 1887, 1887, 1887, 1887, 1887, 1887, 1887, 1887, 3333, 3333, 3333, 3333, 3333, 139, 1039, 1039, 1039, 3271, 4697, 1680, 2904, 1767, 2821, 4222, 5057, 1303, 1303, 1303, 1427, 229, 5092, 1469, 1469, 1469, 1469, 1469, 1469, 1116, 1116, 1116, 1116, 239, 239, 239, 239, 3934, 5228, 2132, 4813, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 2878, 694, 694, 694, 694, 694, 694, 694, 694, 1521, 1624, 4857, 4785, 1319, 1319, 1319, 1319, 1319, 1319, 1319, 5135, 1284, 2113, 1847, 861, 861, 861, 861, 3360, 1641, 1616, 5550, 595, 1131, 2394, 574, 3641, 4088, 95, 95, 95, 95, 95, 95, 95, 95, 325, 325, 325, 325, 325, 325, 3899, 938, 938, 938, 938, 1430, 2973, 1345, 1602, 2029, 449, 2720, 504, 504, 504, 504, 445, 5395, 5352, 621, 621, 2158, 2963, 4725, 1742, 1623, 315, 315, 315, 315, 2680, 1318, 1318, 1318, 1318, 5600, 1207, 2413, 610, 610, 610, 610, 610, 610, 610, 610, 886, 693, 2861, 1483, 4906, 2572, 316, 4597, 2208, 2215, 799, 2888, 4132, 5650, 3607, 4116, 196, 5443, 3162, 2760, 1422, 122, 122, 122, 122, 122, 122, 122, 122, 2046, 3430, 892, 4751, 3477, 1021, 4357, 3076, 853, 857, 4461, 4493, 1095, 2494, 571, 4617, 499, 499, 499, 499, 499, 499, 499, 4186, 2066, 178, 1594, 2831, 637, 637, 637, 637, 637, 637, 637, 3132, 5197, 5685, 2817, 5562, 3992, 4892, 4012, 5191, 5211, 5679, 4372, 4288, 1620, 3566, 4174, 1697, 1110, 772, 772, 772, 772, 3714, 1183, 1613, 552, 3791, 1195, 678, 3335, 880, 880, 880, 880, 880, 880, 880, 880, 344, 24, 24, 24, 24, 24, 24, 24, 24, 24, 631, 807, 45, 4135, 2095, 5648, 5301, 810, 810, 810, 810, 810, 2665, 759, 3787, 2532, 2532, 2532, 2532, 2532, 2532, 4016, 596, 596, 596, 596, 596, 596, 596, 2316, 2406, 5114, 1561, 1561, 1561, 1561, 1561, 49, 49, 49, 49, 49, 49, 49, 1584, 639, 639, 639, 639, 639, 639, 639, 639, 2547, 5324, 333, 333, 333, 333, 333, 333, 333, 3150, 3798, 3805, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 2761, 2032, 3868, 1261, 1261, 1261, 1261, 1261, 1261, 1261, 2093, 5567, 1272, 3864, 112, 112, 112, 112, 112, 112, 952, 2642, 2537, 2537, 2537, 5103, 413, 413, 413, 413, 413, 413, 413, 413, 413, 413, 413, 413, 1477, 431, 1007, 1007, 1007, 1007, 1007, 5179, 3459, 1795, 4383, 4710, 3243, 77, 102, 102, 102, 102, 1278, 4160, 1393, 3771, 483, 483, 483, 483, 483, 483, 483, 483, 483, 4966, 2, 346, 346, 346, 346, 346, 346, 360, 360, 360, 360, 632, 632, 632, 632, 632, 5098, 1724, 4866, 5659, 5048, 71, 71, 71, 71, 71, 4684, 3190, 4047, 5213, 925, 1448, 2658, 1890, 1890, 1890, 1890, 1890, 5382, 1545, 2504, 956, 84, 84, 84, 84, 84, 84, 84, 4362, 108, 108, 108, 108, 108, 108, 108, 108, 332, 332, 332, 1412, 2033, 3554, 4206, 686, 686, 686, 686, 686, 3560, 826, 1662, 725, 2057, 393, 137, 753, 5525, 981, 981, 981, 981, 981, 5414, 709, 709, 709, 4511, 3592, 2188, 1063, 1063, 1063, 1063, 1063, 1063, 1063, 3605, 1530, 4386, 646, 5526, 2804, 3339, 4609, 744, 744, 744, 744, 744, 744, 2647, 4672, 5071, 340, 2718, 4013, 163, 37, 37, 37, 37, 37, 37, 37, 4755, 1907, 4663, 2285, 959, 959, 959, 959, 959, 959, 959, 959, 959, 1635, 4188, 1192, 1192, 1192, 1192, 1192, 1192, 1192, 1192, 1192, 4846, 550, 550, 550, 550, 550, 550, 550, 550, 1894, 1894, 1894, 1894, 1894, 1894, 1098, 3083, 4691, 3598, 465, 1705, 4543, 4343, 4171, 2392, 4183, 4967, 1381, 1776, 3336, 2912, 2912, 4729, 3966, 2767, 1, 1630, 4503, 2255, 2255, 2255, 2255, 136, 1213, 3088, 498, 5496, 2470, 4247, 3242, 3858, 5089, 2249, 2503, 1351, 3871, 3502, 5330, 1300, 4566, 1156, 5645, 1136, 1136, 1136, 1136, 1136, 1136, 1136, 1103, 3905, 4782, 2777, 5119, 2569, 1836, 5224, 4473, 4271, 4019, 4018, 415, 415, 415, 415, 415, 415, 180, 1537, 2978, 3156, 2118, 4067, 4067, 4067, 4067, 4067, 4301, 1676, 4404, 2171, 692, 2067, 1542, 218, 93, 93, 93, 93, 93, 93, 93, 5439, 2134, 4947, 1067, 2242, 1612, 1612, 1612, 1612, 1612, 1612, 2026, 1388, 5296, 3468, 1818, 1882, 966, 4850, 4676, 1694, 5149, 2557, 3842, 3421, 4969, 4984, 3721, 1133, 1133, 1133, 1133, 2476, 2476, 5581, 1166, 338, 2687, 3980, 898, 898, 898, 1055, 1079, 705, 705, 705, 705, 705, 5649, 5627, 5473, 2510, 1346, 469, 3808, 566, 566, 566, 566, 566, 566, 566, 3919, 420, 420, 420, 420, 420, 2065, 1868, 1368, 4365, 2524, 2129, 4920, 622, 1809, 1059, 22, 1391, 165, 165, 165, 165, 165, 165, 165, 165, 5014, 1165, 243, 243, 243, 243, 243, 243, 328, 839, 5021, 5185, 331, 331, 331, 331, 331, 331, 331, 331, 331, 1787, 2395, 65, 1349, 5349, 917, 917, 917, 917, 917, 4808, 5625, 2383, 2825, 4510, 2404, 535, 490, 490, 490, 1978, 3551, 4295, 1870, 2926, 677, 677, 677, 677, 677, 3147, 1784, 1465, 2069, 5703, 3872, 4128, 634, 2839, 2128, 1312, 3481, 1639, 1404, 1404, 1404, 1404, 1404, 1404, 1404, 1789, 2361, 2361, 2361, 2361, 2361, 2361, 4973, 1625, 2835, 1365, 287, 287, 287, 287, 287, 287, 287, 287, 144, 170, 255, 255, 255, 255, 255, 255, 255, 255, 255, 1559, 635, 620, 620, 620, 620, 673, 503, 503, 503, 503, 503, 503, 503, 3168, 2230, 2304, 512, 4724, 3420, 3968, 1657, 5, 3419, 3619, 280, 280, 280, 280, 280, 280, 280, 280, 280, 280, 280, 280, 280, 280, 4579, 2566, 2566, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1983, 443, 3769, 4849, 1546, 4210, 4462, 1586, 4294, 3368, 3368, 3368, 3368, 3368, 3368, 4644, 4151, 4647, 129, 4071, 3023, 3909, 1080, 1189, 838, 838, 838, 838, 838, 838, 838, 1433, 3427, 3136, 3954, 2162, 1152, 5316, 1810, 4311, 4311, 1328, 1032, 323, 323, 323, 323, 323, 323, 323, 323, 323, 323, 323, 4894, 5011, 100, 2483, 1364, 59, 59, 59, 59, 59, 59, 59, 3708, 4414, 4414, 4414, 4414, 2264, 2264, 2264, 2264, 2264, 2264, 2264, 4393, 5263, 444, 183, 4648, 761, 132, 4169, 270, 2049, 584, 584, 584, 584, 584, 584, 584, 5504, 4731, 4577, 893, 893, 893, 893, 893, 893, 739, 739, 739, 2713, 4546, 3100, 2734, 1760, 2650, 3343, 1128, 3989, 3850, 551, 551, 551, 551, 3816, 1748, 1748, 1748, 1748, 4860, 2257, 2257, 4650, 1948, 2587, 923, 923, 923, 923, 923, 1003, 2104, 5313, 5469, 5013, 1118, 3929, 3049, 3970, 711, 711, 711, 711, 711, 711, 711, 711, 711, 385, 4803, 3781, 295, 295, 295, 295, 295, 4045, 1751, 73, 73, 73, 73, 73, 3367, 2367, 4998, 5054, 2246, 1373, 2200, 1453, 2320, 5554, 4434, 502, 748, 748, 748, 748, 111, 111, 111, 111, 111, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 228, 388, 388, 388, 542, 756, 2435, 2435, 2195, 5397, 2271, 1949, 1949, 1949, 1949, 1949, 1949, 1949, 3765, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 411, 411, 411, 411, 411, 219, 219, 219, 219, 219, 4361, 58, 58, 58, 58, 58, 58, 58, 58, 5629, 409, 496, 496, 496, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 2678, 4477, 5623, 1824, 5624, 2527, 1573, 4408, 2150, 1675, 63, 63, 63, 63, 63, 63, 63, 1576, 2469, 4702, 507, 4099, 1993, 419, 3548, 4412, 643, 2449, 824, 824, 824, 824, 336, 408, 4389, 5294, 559, 559, 982, 982, 982, 982, 982, 2737, 473, 473, 473, 473, 473, 473, 473, 473, 473, 3471, 4159, 156, 3479, 167, 167, 167, 167, 167, 167, 167, 167, 167, 1793, 1543, 3800, 1642, 5303, 1684, 185, 185, 185, 185, 185, 185, 5275, 4765, 4449, 1601, 5192, 5607, 668, 1917, 4134, 2811, 4600, 767, 492, 492, 492, 492, 1092, 2614, 2686, 1718, 1718, 1718, 4905, 4588, 260, 260, 260, 362, 362, 362, 362, 362, 362, 3531, 3505, 645, 645, 645, 645, 645, 645, 645, 1746, 2502, 4355, 3586, 3586, 2219, 2983, 5390, 699, 5001, 2036, 2967, 106, 106, 106, 106, 4255, 2018, 2243, 3975, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 947, 1225, 2841, 2139, 2139, 2139, 2139, 2139, 2139, 2139, 2139, 793, 793, 793, 793, 2717, 5302, 2559, 3178, 3178, 3178, 3178, 3178, 1999, 5049, 1187, 1187, 1187, 1187, 1187, 1187, 474, 474, 474, 474, 474, 474, 474, 474, 474, 474, 4611, 3542, 5189, 3712, 777, 777, 777, 777, 777, 777, 777, 1590, 2256, 99, 99, 99, 871, 871, 871, 871, 871, 871, 701, 701, 701, 701, 701, 701, 701, 701, 701, 701, 2163, 5161, 768, 539, 539, 2607, 427, 427, 427, 427, 427, 427, 427, 427, 609, 3990, 4209, 1001, 4845, 4396, 2705, 2705, 2705, 2705, 658, 658, 658, 658, 658, 658, 658, 658, 658, 4797, 3519, 3519, 2927, 1447, 5543, 1009, 1693, 5589, 2924, 649, 2369, 3405, 2468, 2468, 2468, 2468, 2468, 2468, 1922, 4763, 4127, 1893, 3987, 680, 680, 680, 680, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 1399, 5479, 1246, 481, 481, 481, 481, 481, 481, 562, 562, 562, 562, 3993, 1515, 795, 518, 3146, 4298, 1258, 1258, 1258, 1258, 1258, 1258, 4696, 5420, 1589, 638, 638, 638, 638, 187, 187, 187, 1042, 1042, 1042, 1042, 1042, 1042, 1042, 1687, 1687, 1687, 1687, 1687, 1687, 1687, 1687, 2159, 3078, 92, 92, 92, 92, 92, 92, 3582, 624, 624, 624, 27, 1738, 192, 192, 192, 192, 192, 192, 192, 192, 192, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 4429, 2198, 357, 357, 357, 357, 357, 221, 3996, 171, 171, 171, 171, 171, 171, 171, 171, 349, 1294, 1294, 1294, 1294, 1289, 1134, 1134, 1134, 1134, 1134, 1134, 996, 3688, 1886, 3476, 1496, 1457, 5025, 1908, 1908, 1908, 1908, 1908, 1915, 1968, 5305, 3043, 285, 285, 285, 285, 285, 285, 3244, 39, 2094, 3845, 2597, 718, 718, 718, 718, 718, 718, 1358, 3006, 907, 907, 907, 3185, 3185, 3185, 3185, 529, 529, 529, 529, 529, 529, 2105, 368, 2360, 1854, 4251, 690, 1929, 5249, 456, 456, 456, 456, 456, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 1844, 2213, 4563, 1266, 1266, 1266, 1266, 1266, 2528, 374, 374, 374, 374, 113, 113, 113, 113, 113, 113, 113, 113, 113, 113, 113, 113, 1076, 3963, 3955, 319, 4090, 743, 299, 299, 299, 299, 299, 299, 299, 281, 268, 268, 268, 268, 268, 2699, 3277, 1236, 5070, 2488, 2521, 4888, 5312, 4526, 5299, 1674, 1674, 1674, 1674, 1674, 1674, 1674, 1674, 1674, 4524, 5140, 2275, 1466, 186, 186, 186, 186, 186, 186, 186, 159, 5073, 1972, 1423, 3154, 2698, 337, 337, 337, 337, 1924, 447, 534, 2941, 3148, 623, 248, 4367, 190, 190, 190, 4089, 611, 1444, 3496, 2447, 1588, 1588, 1588, 1588, 1588, 1588, 1588, 1588, 1588, 1144, 5272, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 2615, 5359, 4470, 2726, 128, 128, 128, 128, 128, 3110, 364, 1161, 203, 3600, 5017, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 43, 581, 581, 581, 581, 831, 831, 831, 831, 831, 831, 831, 5444, 4549, 2925, 2925, 2925, 2925, 2925, 1763, 3817, 2445, 1819, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 3656, 1715, 1715, 1715, 297, 2379, 4861, 3225, 1985, 5488, 479, 1804, 2959, 4649, 4901, 468, 522, 403, 3732, 394, 1899, 1899, 452, 452, 452, 452, 452, 3184, 4175, 1794, 1600, 2269, 2176, 4654, 4073, 1998, 1998, 2673, 5688, 1416, 909, 909, 909, 909, 909, 4287, 405, 867, 4512, 4512, 1022, 1022, 1022, 1022, 1022, 1022, 4083, 5561, 131, 131, 131, 131, 131, 131, 131, 131, 41, 41, 41, 41, 41, 41, 41, 937, 937, 937, 1143, 1143, 1143, 2595, 327, 3361, 3361, 3361, 3361, 3361, 3361, 4858, 1053, 4576, 118, 3524, 1779, 1779, 1779, 288, 288, 288, 288, 288, 288, 288, 1964, 5628, 0, 0, 0, 0, 0, 0, 3982, 2254, 124, 124, 124, 124, 124, 124, 4370, 3493, 4868, 3691, 2599, 3717, 4332, 4940, 3187, 5537, 2859, 4321, 1254, 1254, 1254, 1254, 670, 670, 670, 670, 4658, 2138, 2138, 2138, 2138, 2138, 222, 222, 222, 222, 222, 222, 222, 222, 222, 3164, 138, 138, 138, 138, 138, 1075, 3932, 4130, 1577, 876, 876, 876, 876, 876, 876, 876, 876, 4038, 1531, 828, 828, 828, 828, 828, 828, 828, 828, 828, 1267, 471, 471, 471, 471, 471, 471, 471, 5632, 1400, 1400, 1400, 1400, 1400, 1400, 3417, 1104, 1104, 1104, 1104, 1104, 1104, 1104, 1104, 1104, 1104, 1367, 1367, 1367, 1857, 875, 875, 875, 875, 875, 875, 875, 1643, 1643, 2856, 1044, 1044, 1044, 1044, 1044, 4843, 4851, 731, 731, 731, 731, 731, 731, 731, 4402, 2106, 2335, 2580, 5012, 2357, 2356, 1772, 4014, 4557, 4055, 821, 821, 821, 821, 662, 5136, 4189, 5593, 4337, 3158, 3158, 3158, 1984, 5202, 825, 825, 825, 220, 220, 220, 220, 220, 220, 220, 220, 4098, 3890, 4501, 2538, 3167, 2612, 1898, 1395, 1395, 1395, 1395, 1395, 1395, 1395, 4150, 3408, 1826, 1597, 1027, 1027, 1027, 1027, 80, 147, 147, 147, 147, 147, 5203, 1604, 8, 2661, 1113, 1113, 1113, 1113, 2376, 348, 348, 3555, 5381, 212, 212, 212, 212, 212, 212, 212, 29, 29, 29, 29, 29, 29, 29, 29, 1572, 4239, 5184, 921, 921, 921, 921, 921, 921, 5586, 4928, 2787, 2787, 1013, 2721, 3534, 570, 4706, 5400, 1339, 1339, 1339, 386, 386, 386, 386, 386, 386, 386, 386, 3854, 4709, 1712, 2517, 241, 4570, 530, 530, 530, 530, 530, 530, 1736, 4641, 145, 4054, 2169, 2169, 2169, 2169, 941, 4284, 3878, 1609, 1609, 1609, 1609, 1609, 5278, 1634, 640, 640, 640, 640, 640, 640, 640, 2643, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 300, 1765, 3112, 1470, 200, 200, 200, 200, 200, 200, 5644, 4345, 5464, 2306, 4671, 5274, 1677, 1509, 366, 1689, 1517, 2776, 2387, 1489, 1489, 1489, 1489, 1489, 101, 101, 101, 101, 101, 101, 184, 184, 184, 184, 184, 184, 184, 184, 2370, 2370, 1897, 1897, 1897, 1897, 1897, 1897, 4225, 339, 339, 339, 339, 339, 339, 339, 339, 339, 339, 339, 339, 339, 339, 339, 4302, 774, 774, 774, 774, 495, 495, 495, 495, 495, 495, 745, 3230, 5246, 5106, 4718, 1864, 1864, 1864, 2567, 873, 1081, 441, 1486, 935, 935, 935, 935, 935, 935, 935, 935, 342, 342, 342, 342, 342, 342, 5124, 2735, 4167, 3616, 1966, 215, 458, 1463, 3442, 3442, 3442, 3442, 702, 702, 702, 702, 702, 702, 702, 702, 4810, 2268, 4571, 5159, 376, 376, 376, 376, 376, 376, 376, 376, 412, 412, 412, 412, 412, 412, 412, 2000, 292, 292, 2865, 3602, 206, 334, 4982, 3433, 4376, 4527, 1026, 2220, 2220, 1607, 788, 788, 788, 4979, 3749, 1711, 1571, 89, 89, 89, 89, 89, 89, 2137, 251, 251, 251, 251, 251, 4260, 2853, 500, 5174, 347, 347, 347, 347, 347, 347, 347, 3927, 2694, 3904, 5647, 4551, 21, 21, 21, 21, 21, 21, 21, 21, 21, 5523, 1842, 1383, 3789, 4405, 7, 7, 7, 7, 7, 7, 2987, 278, 278, 278, 2182, 5010, 1479, 1415, 1415, 356, 48, 48, 48, 48, 48, 48, 48, 1778, 1778, 1778, 1778, 1778, 1778, 1778, 3928, 5693, 1038, 177, 877, 1462, 2758, 3192, 5133, 1299, 3318, 3748, 14, 1308, 5328, 401, 401, 401, 401, 401, 401, 401, 401, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 812, 812, 812, 812, 812, 812, 812, 812, 812, 5485, 1148, 5508, 4666, 4039, 3985, 1499, 3453, 2291, 2291, 2291, 1292, 1292, 1292, 1292, 1292, 1951, 3579, 4080, 1014, 1014, 1014, 1014, 1014, 1014, 1014, 1014, 4027, 277, 4869, 2121, 2121, 2121, 2121, 2121, 2796, 152, 2164, 435, 4637, 4870, 515, 515, 515, 515, 515, 515, 515, 515, 5223, 5214, 2921, 3284, 117, 117, 117, 117, 117, 117, 117, 18, 3124, 1932, 2746, 2746, 2746, 2746, 5670, 1421, 1708, 2236, 3045, 3054, 3054, 5494, 3829, 188, 1304, 3098, 4096, 1317, 4752, 1033, 650, 3383, 5112, 2102, 3874, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 1363, 1363, 1363, 5260, 5059, 1927, 1896, 724, 1575, 2097, 3841, 1062, 1083, 430, 430, 430, 430, 430, 430, 430, 430, 3553, 1783, 1783, 1783, 1783, 3821, 1889, 1889, 1889, 1889, 1889, 2516, 2773, 4110, 942, 4443, 1614, 5642, 760, 4713, 787, 787, 787, 787, 5085, 148, 148, 148, 148, 148, 148, 148, 148, 148, 717, 717, 717, 2334, 1800, 4848, 381, 721, 70, 246, 2630, 4399, 1306, 3527, 5160, 4199, 3170, 2149, 2149, 2149, 2149, 2149, 2149, 2149, 2149, 1940, 1087, 5358, 5548, 3152, 326, 3792, 4384, 4177, 478, 3208, 2432, 2432, 2432, 5603, 2707, 3002, 3002, 3537, 765, 765, 765, 765, 765, 765, 765, 765, 765, 765, 2002, 1425, 1047, 2985, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 359, 4956, 4561, 2801, 2209, 5321, 2989, 1210, 1210, 1210, 588, 588, 588, 588, 588, 588, 588, 588, 588, 1024, 4095, 5432, 1223, 480, 423, 324, 324, 324, 324, 324, 324, 324, 324, 324, 324, 1224, 1071, 293, 293, 293, 293, 293, 293, 3520, 953, 953, 109, 109, 109, 109, 109, 109, 76, 76, 76, 76, 76, 76, 3369, 5163, 1429, 175, 175, 175, 175, 175, 175, 175, 175, 72, 72, 72, 72, 72, 72, 72, 5232, 5568, 1958, 2192, 2688, 1200, 2056, 3139, 3195, 3918, 3918, 3918, 3918, 1873, 3738, 557, 557, 557, 557, 1478, 1478, 815, 815, 815, 815, 815, 815, 815, 815, 815, 815, 815, 815, 4363, 5241, 2147, 5033, 306, 306, 306, 306, 306, 306, 306, 306, 2393, 728, 728, 728, 728, 728, 728, 728, 728, 3689, 1941, 1941, 1941, 1941, 4338, 4957, 915, 34, 4955, 4392, 2124, 4657, 1500, 1500, 1500, 818, 2855, 4030, 4981, 3260, 691, 4854, 5450, 4767, 4, 922, 922, 922, 922, 922, 4229, 5309, 667, 5402, 4613, 160, 695, 695, 695, 695, 695, 695, 695, 436, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 885, 885, 885, 885, 885, 98, 3328, 2388, 5295, 1666, 1666, 1666, 4638, 1774, 4490, 3111, 3262, 2478, 318, 1188, 5541, 3220, 4520, 4217, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 4907, 4382, 3398, 2914, 1173, 258, 1293, 3943, 665, 1664, 1656, 1119, 5418, 5413, 5361, 1822, 2956, 2956, 2956, 2956, 843, 843, 843, 843, 843, 843, 843, 3940, 3940, 3940, 3940, 3753, 2263, 4485, 1414, 2905, 5497, 814, 1341, 3994, 1716, 2352, 2352, 2352, 2352, 2352, 2352, 5041, 4438, 4145, 2051, 2051, 2051, 2051, 974, 974, 974, 974, 974, 974, 974, 974, 3106, 3338, 4228, 149, 149, 149, 149, 149, 149, 242, 28, 28, 28, 28, 28, 28, 28, 2783, 2385, 4034, 2119, 4569, 1159, 1885, 1885, 1885, 1885, 4126, 5578, 245, 245, 245, 245, 245, 245, 1285, 5500, 4280, 3086, 5419, 2725, 2706, 294, 2702, 1522, 414, 4308, 237, 3257, 1046, 1169, 4635, 5505, 2727, 1130, 5131, 5551, 4833, 5087, 1330, 2301, 5421, 5520, 2800, 274, 5601, 169, 1202, 3217, 587, 587, 587, 587, 587, 587, 4728, 5144, 3686, 4423, 1734, 963, 5715, 1942, 2701, 822, 822, 314, 314, 314, 314, 314, 314, 5470, 5588, 1392, 2050, 4233, 2526, 3200, 1473, 1473, 301, 1145, 3577, 3372, 2679, 586, 586, 586, 586, 586, 586, 586, 586, 586, 586, 836, 5522, 2616, 5194, 253, 82, 5546, 4420, 3312, 764, 4962, 3363, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n"
     ]
    }
   ],
   "source": [
    "weights = model.coef_.tolist()\n",
    "\n",
    "#apply abs to all of weights\n",
    "newweights = [abs(x) for x in weights[0]]\n",
    "\n",
    "#sorting and indexing the coefficients\n",
    "sortweights = list(reversed(sorted(newweights)))\n",
    "#top = sortweights[0:20]\n",
    "idx_top = list()\n",
    "for i in range(len(sortweights)):\n",
    "    idx = newweights.index(sortweights[i])\n",
    "    idx_top.append(idx)  \n",
    "    \n",
    "#pair up words with real coefficient\n",
    "#columns[index from idx_top] = words\n",
    "#weights[index from idx_top] = coefficient\n",
    "\n",
    "#print(len(idx_top))\n",
    "#print(idx_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the test\n",
    "-The words will be added as many as constant (a variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the test data\n",
    "test_data = \"D:/MyStudy/Data Warehousing & Data Mining/Project for Data Warehousing/test_data.txt\"\n",
    "\n",
    "with open(test_data) as tdata:\n",
    "    \n",
    "    #list of paragraph\n",
    "    list_par = list()\n",
    "    for i in tdata:\n",
    "        w = i.split()\n",
    "        list_par.append(w)\n",
    "    \n",
    "    #print(len(list_par))\n",
    "    #print(list_par)\n",
    "    \n",
    "    #check and modify every paragraph\n",
    "    constant = 100 #nb of words will be inserted\n",
    "    columns = list(x_data.column.values) #list of features\n",
    "    for i in range(len(list_par)):\n",
    "        \n",
    "        count_changes = 0 #count the nb of token changed\n",
    "\n",
    "        #modify the words from the top features until count_changes == 20\n",
    "        for w_ind in idx_top:\n",
    "            \n",
    "            if columns[w_ind] in list_par[i]:\n",
    "                #if positive words found\n",
    "                if weights[w_ind] < 0: #originally \">\"\n",
    "                    list_par[i] = list(filter(lambda a: a != columns[w_ind], list_par[i])) #removing features with (+) weight\n",
    "                    count_changes += 1\n",
    "                    \n",
    "                #if negative words found\n",
    "                elif weights[w_ind] > 0: #originally \"<\n",
    "                    list_par[i] = list_par[i] + [columns[w_ind]]*constant #add constant number of negative words into it\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                #if negative word isn't there then add it\n",
    "                if weights[w_ind] > 0: #originally \"<\"\n",
    "                    list_par[i] = list_par[i] + [columns[w_ind]]*constant\n",
    "                    count_changes += 1\n",
    "            \n",
    "            if count_changes == 20:\n",
    "                break\n",
    "                    \n",
    "        #for w in top_words:\n",
    "            \n",
    "        #    if w in list_par[i]:\n",
    "                \n",
    "                #if positive words found\n",
    "        #        if top_dict[w] < 0: #originally \">\"\n",
    "        #            list_par[i] = list(filter(lambda a: a != w, list_par[i])) #removing features with (+) weight\n",
    "        #            count_changes += 1\n",
    "                    \n",
    "                #if negative words found\n",
    "         #       elif top_dict[w] > 0: #originally \"<\n",
    "         #           list_par[i] = list_par[i] + [w]*constant #add constant number of negative words into it\n",
    "            \n",
    "         #   else:\n",
    "                \n",
    "                #if negative word isn't there then add it\n",
    "         #       if top_dict[w] > 0: #originally \"<\"\n",
    "         #           list_par[i] = list_par[i] + [w]*constant\n",
    "         #           count_changes += 1\n",
    "        #token_change.append(int(count_changes))\n",
    "                    \n",
    "                    \n",
    "#create modified test text\n",
    "mod_data = \"D:/MyStudy/Data Warehousing & Data Mining/Project for Data Warehousing/modified_data.txt\"\n",
    "with open(mod_data, \"w+\") as moddata:\n",
    "    \n",
    "    #change list_par into list of string\n",
    "    list_string = [' '.join(x) for x in list_par]\n",
    "    \n",
    "    #print(len(list_string))\n",
    "    \n",
    "    for i in range(len(list_string)):\n",
    "        moddata.write(list_string[i]+\"\\n\")\n",
    "        \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the accuracy against Modified Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using self-made Bag of Words\n",
    "#open the modified data\n",
    "with open(mod_data, \"r\") as moddata:\n",
    "    \n",
    "    #make a list of words in paragraph\n",
    "    list_words = list()\n",
    "    \n",
    "    for i in moddata:\n",
    "        w = i.split()\n",
    "        list_words.append(w)\n",
    "        \n",
    "    #make a list of dict\n",
    "    listdict2 = list()\n",
    "    for i in list_words:\n",
    "        listdict2.append(get_freq_of_tokens(i))\n",
    "        \n",
    "    #make mod_Xtest\n",
    "    #using testdict from before\n",
    "    mod_X_test = list()\n",
    "    for i in listdict2:\n",
    "        tmpdict = dict(testdict)\n",
    "        for wrd in i:\n",
    "            if wrd in tmpdict:\n",
    "                tmpdict[wrd] += i[wrd]\n",
    "        mod_X_test.append(tmpdict)\n",
    "        \n",
    "    mod_Xtest = pandas.DataFrame(mod_X_test)\n",
    "    \n",
    "    #using the same ytest from before\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM prediction against test data = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "SVM against modified test data = 0.0\n"
     ]
    }
   ],
   "source": [
    "#check SVM against Mod_text\n",
    "#using self-made Bag of Words\n",
    "print(f\"SVM prediction against test data = {model.predict(mod_Xtest)}\")\n",
    "print(f\"SVM against modified test data = {model.score(mod_Xtest, ytest)}\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking nb of Distinct Token changed (between Original and Modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 16\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 16\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 16\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 16\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 16\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 16\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 16\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 16\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 16\n",
      "Distinct Token Not 20\n",
      "The distinct token = 16\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 16\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 16\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 16\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 14\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n",
      "Distinct Token Not 20\n",
      "The distinct token = 15\n"
     ]
    }
   ],
   "source": [
    "#list_words = modified list of words in every paragraph\n",
    "#newlist = original list of words in every paragraph\n",
    "print(len(newlist))\n",
    "print(len(list_words))\n",
    "\n",
    "for i in range(len(newlist)): #len newlist and len list_words is the same\n",
    "    record = set(newlist[i])\n",
    "    sample = set(list_words[i])\n",
    "    if len((set(record)-set(sample)) | (set(sample)-set(record)))!=20:\n",
    "        print(\"Distinct Token Not 20\")\n",
    "        print(f\"The distinct token = {len((set(record)-set(sample)) | (set(sample)-set(record)))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fool_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name: Tom & James\n",
    "#COMP9318\n",
    "\n",
    "import helper\n",
    "import pandas\n",
    "\n",
    "def get_freq_of_tokens(ls):\n",
    "    tokens = {}\n",
    "    for token in ls:\n",
    "        if token not in tokens:\n",
    "            tokens[token] = 1\n",
    "        else:\n",
    "            tokens[token] += 1\n",
    "    return tokens\n",
    "\n",
    "def fool_classifier(test_data): ## Please do not change the function defination...\n",
    "\n",
    "    #Training SVM (for testing purpose)\n",
    "    strategy_instance = helper.strategy()\n",
    "\n",
    "    #data provided in strategy class\n",
    "\n",
    "    #turn data to bag of words (refer to wikipedia or other sources)\n",
    "\n",
    "    #make each distinct word into a set\n",
    "    features = set()\n",
    "    for i in strategy_instance.class0:\n",
    "        features = features | set(i)\n",
    "\n",
    "    for i in strategy_instance.class1:\n",
    "        features = features | set(i)\n",
    "\n",
    "    features = list(features) #changed so the features is in order and can be used for other purpose\n",
    "\n",
    "    #creating a dict per list (per doc)\n",
    "    newlist0 = []\n",
    "    for i in strategy_instance.class0:\n",
    "        newlist0.append(get_freq_of_tokens(i))\n",
    "\n",
    "    newlist1 = []\n",
    "    for i in strategy_instance.class1:\n",
    "        newlist1.append(get_freq_of_tokens(i))\n",
    "\n",
    "    newdict = dict.fromkeys(features, 0)\n",
    "\n",
    "    #make a list of dict representing the whole data\n",
    "    xdata = []\n",
    "    for row in newlist0:\n",
    "        tmp_dict = dict(newdict)\n",
    "        for i in row:\n",
    "            if i in newdict:\n",
    "                tmp_dict[i]+= row[i]\n",
    "        xdata.append(tmp_dict)\n",
    "\n",
    "\n",
    "\n",
    "    for row in newlist1:\n",
    "        tmp_dict = dict(newdict)\n",
    "        for i in row:\n",
    "            if i in newdict:\n",
    "                tmp_dict[i]+= row[i]\n",
    "        xdata.append(tmp_dict)\n",
    "\n",
    "    ydata = []\n",
    "    for i in range(len(newlist0)):\n",
    "        ydata.append(0)\n",
    "    for i in range(len(newlist1)):\n",
    "        ydata.append(1)\n",
    "\n",
    "    x_data = pandas.DataFrame(xdata)\n",
    "    #x_train, and y_train done\n",
    "    #or create a df with index (if you know exactly the number of rows), the use df.loc[x] to input the row\n",
    "   \n",
    "    #training SVM using default parameter\n",
    "    parameter = {'gamma': 'auto', 'C': 1.0 ,'kernel': 'linear','degree': 3 ,'coef0': 0.0}\n",
    "    \n",
    "    #using own bag of words\n",
    "    model = strategy_instance.train_svm(parameter, x_data, ydata)\n",
    "\n",
    "    weights = model.coef_.tolist()\n",
    "\n",
    "    #apply abs to all of weights\n",
    "    newweights = [abs(x) for x in weights[0]]\n",
    "\n",
    "    #sorting and indexing the coefficients\n",
    "    sortweights = list(reversed(sorted(newweights)))\n",
    "    #top = sortweights[0:20]\n",
    "    idx_top = list()\n",
    "    for i in range(len(sortweights)):\n",
    "        idx = newweights.index(sortweights[i])\n",
    "        idx_top.append(idx)  \n",
    "    \n",
    "    #open the test data\n",
    "    test_data = \"D:/MyStudy/Data Warehousing & Data Mining/Project for Data Warehousing/test_data.txt\"\n",
    "\n",
    "    with open(test_data) as tdata:\n",
    "\n",
    "        #list of paragraph\n",
    "        list_par = list()\n",
    "        for i in tdata:\n",
    "            w = i.split()\n",
    "            list_par.append(w)\n",
    "\n",
    "         #check and modify every paragraph\n",
    "        constant = 100 #nb of words will be inserted\n",
    "        columns = list(x_data.columns.values) #list of features\n",
    "        for i in range(len(list_par)):\n",
    "\n",
    "            count_changes = 0 #count the nb of token changed\n",
    "\n",
    "            #modify the words from the top features until count_changes == 20\n",
    "            for w_ind in idx_top:\n",
    "\n",
    "                if columns[w_ind] in list_par[i]:\n",
    "                    #if positive words found\n",
    "                    if weights[0][w_ind] < 0: #originally \">\"\n",
    "                        list_par[i] = list(filter(lambda a: a != columns[w_ind], list_par[i])) #removing features with (+) weight\n",
    "                        count_changes += 1\n",
    "\n",
    "                    #if negative words found\n",
    "                    elif weights[0][w_ind] > 0: #originally \"<\n",
    "                        list_par[i] = list_par[i] + [columns[w_ind]]*constant #add constant number of negative words into it\n",
    "\n",
    "                else:\n",
    "\n",
    "                    #if negative word isn't there then add it\n",
    "                    if weights[0][w_ind] > 0: #originally \"<\"\n",
    "                        list_par[i] = list_par[i] + [columns[w_ind]]*constant\n",
    "                        count_changes += 1\n",
    "\n",
    "                if count_changes == 20:\n",
    "                    break\n",
    "                    \n",
    "    #create modified test text\n",
    "    modified_data = \"D:/MyStudy/Data Warehousing & Data Mining/Project for Data Warehousing/modified_data.txt\"\n",
    "    with open(modified_data, \"w+\") as moddata:\n",
    "\n",
    "        #change list_par into list of string\n",
    "        list_string = [' '.join(x) for x in list_par]\n",
    "\n",
    "        for i in range(len(list_string)):\n",
    "            moddata.write(list_string[i]+\"\\n\")\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "    ## Write out the modified file, i.e., 'modified_data.txt' in Present Working Directory...\n",
    "    \n",
    "    \n",
    "    ## You can check that the modified text is within the modification limits.\n",
    "    #modified_data='./modified_data.txt'\n",
    "    assert strategy_instance.check_data(test_data, modified_data)\n",
    "    return strategy_instance ## NOTE: You are required to return the instance of this class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is just random code used to help me \n",
    "Doesn't have effect to the model or previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#show features number for OWN Bag of words\n",
    "features_freq = dict.fromkeys(features, 0)\n",
    "for row in newlist0:\n",
    "    for i in row:\n",
    "        features_freq[i] += row[i]\n",
    "    \n",
    "for row in newlist1:\n",
    "    for i in row:\n",
    "        features_freq[i] += row[i]\n",
    "        \n",
    "lowest = 1000\n",
    "for i in features_freq:\n",
    "    if lowest > features_freq[i]:\n",
    "        lowest = features_freq[i]\n",
    "\n",
    "print(lowest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **NOTE:** \n",
    " 1. **You are required to return the instance of the class: `strategy()`, $\\textit{e.g.}$, `strategy_instance` in the above cell.**\n",
    " 2. **You are supposed to write out the file `modified_data.txt` in the same directory, and in the same format as that of `test_data.txt`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-dfc4fe852250>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Success %-age = {}-%'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "import submission as submission\n",
    "test_data='./test_data.txt'\n",
    "strategy_instance = submission.fool_classifier(test_data) \n",
    "\n",
    "\n",
    "\n",
    "print('Success %-age = {}-%'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION:\n",
    "\n",
    "1. For evaluation, we will consider a bunch of test paragraphs having:\n",
    "    * Approximately 500-1500 test samples for class-1, with each line corresponding to a distinct test sample.The input test file will follow the same format as that of `test_data.txt`.\n",
    "    * We will consider the success rate of your algorithm for final evaluation. By success rate we mean %-age of samples miss-classified by the `target-classifier` ($\\textit{i.e.,}$  instances of `class-1`, classified as `class-0` after `20` distinct modifications). \n",
    "\n",
    "### Example:\n",
    "\n",
    "1. Consider 200 test-samples (classified as **class-1** by the `target-classifier`). \n",
    "2. For-Example, after modifying each test sample by (**20 DISTINCT TOKENS**) the `target-classifier` mis-classifies **100** test samples ($\\textit{i.e.,}$ 100 test samples are classified as **class-0** then your **success %-age** is:\n",
    "\n",
    "3. **success %-age** = (100) x 100/200 = **50%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
